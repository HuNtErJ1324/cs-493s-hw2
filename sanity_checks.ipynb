{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob5IepnmN4PQ",
        "outputId": "e6f0b217-6fe8-4295-f933-098dc43f4b9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/hw2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiSyWqjFN6yZ",
        "outputId": "f606a4a6-faae-4eee-dd12-9b8ec6fccd5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/hw2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import model\n",
        "import utils"
      ],
      "metadata": {
        "id": "c2jECcsRJOFe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHriz22ltPl5",
        "outputId": "c619914a-988b-4be9-a7b0-b394820dadc1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8a70722ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"I love machine learning\"]\n",
        "tokenizer = utils.Tokenizer(24)\n",
        "tokenizer.build_tokenization(text)\n",
        "tokenized_text = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "rYinPsynNcZj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_layer_config = model.GPTConfig\n",
        "one_layer_config.vocab_size = tokenizer.vocab_size\n",
        "one_layer_config.n_layer = 1\n",
        "small_gpt = model.GPT(one_layer_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ZvTo0fYUzr",
        "outputId": "94768357-538e-4b23-b2a3-b12ff116fcc7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 7.10M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular inference sampling"
      ],
      "metadata": {
        "id": "JRuPcgbBRdN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic train loop on single vector\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedded, mask = torch.from_numpy(tokenized_text[0]),torch.from_numpy(tokenized_text[1])\n",
        "loss = 1 # initial value for loop condition\n",
        "small_gpt.train()\n",
        "small_gpt.to(device)\n",
        "optimizer = small_gpt.configure_optimizers(learning_rate=1e-5, weight_decay=0.0, betas =(0.9,0.95),device_type=device)\n",
        "embedded = embedded.to(device)\n",
        "mask = mask.to(device)\n",
        "i = 0\n",
        "while loss > 1e-16:\n",
        "  optimizer.zero_grad()\n",
        "  if i%100 == 1:\n",
        "    print(\"Step:\",i,\" Loss:\",loss.item())\n",
        "  pred_logits = small_gpt(embedded,attn_mask=mask)\n",
        "  loss_per_token = torch.nn.functional.cross_entropy(pred_logits.flatten(end_dim=1),embedded.view(-1),reduction=\"none\")\n",
        "  loss = (loss_per_token * mask.view(-1)).sum() / mask.sum()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  i += 1\n",
        "print(\"Step:\",i,\" Loss:\",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys0DMvLo6tZv",
        "outputId": "94c0142a-a8c2-4149-e672-cabfd371c19d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 6, with 7,875,840 parameters\n",
            "num non-decayed parameter tensors: 10, with 11,520 parameters\n",
            "using fused AdamW: True\n",
            "Step: 1  Loss: 1.8072611093521118\n",
            "Step: 101  Loss: 0.001599593204446137\n",
            "Step: 201  Loss: 0.00018330039165448397\n",
            "Step: 301  Loss: 2.1379692043410614e-05\n",
            "Step: 401  Loss: 3.81987319997279e-06\n",
            "Step: 501  Loss: 1.2853861335315742e-06\n",
            "Step: 601  Loss: 6.37510311207734e-07\n",
            "Step: 701  Loss: 3.4207874932690174e-07\n",
            "Step: 801  Loss: 2.0732048255922564e-07\n",
            "Step: 901  Loss: 1.3994133496453287e-07\n",
            "Step: 1001  Loss: 7.256217315898539e-08\n",
            "Step: 1101  Loss: 5.7013131993244315e-08\n",
            "Step: 1201  Loss: 4.146409438021692e-08\n",
            "Step: 1301  Loss: 5.7013131993244315e-08\n",
            "Step: 1379  Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_gpt.eval()\n",
        "with torch.no_grad():\n",
        "  sampling = torch.zeros_like(embedded)\n",
        "  sampling[0,0:3] = embedded[0,0:3]\n",
        "  for i in range(23):\n",
        "    pred_logits = small_gpt(sampling)\n",
        "    sampling[0,i] = pred_logits.argmax(dim=2)[0,i]\n",
        "    print(tokenizer.untokenize(sampling.cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmuEoZC2qIRm",
        "outputId": "27195746-ebf0-477e-b8c9-e68382e9c5c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I l']\n",
            "['I l']\n",
            "['I l']\n",
            "['I lo']\n",
            "['I lov']\n",
            "['I love']\n",
            "['I love ']\n",
            "['I love m']\n",
            "['I love ma']\n",
            "['I love mac']\n",
            "['I love mach']\n",
            "['I love machi']\n",
            "['I love machin']\n",
            "['I love machine']\n",
            "['I love machine ']\n",
            "['I love machine l']\n",
            "['I love machine le']\n",
            "['I love machine lea']\n",
            "['I love machine lear']\n",
            "['I love machine learn']\n",
            "['I love machine learni']\n",
            "['I love machine learnin']\n",
            "['I love machine learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model:\n",
        "torch.save(small_gpt.state_dict(), \"sanity_checks.pth\")"
      ],
      "metadata": {
        "id": "KlVkGPsmUcKC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Same but with first three masked"
      ],
      "metadata": {
        "id": "MeaglVqMQ5Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_layer_config = model.GPTConfig\n",
        "one_layer_config.vocab_size = tokenizer.vocab_size\n",
        "one_layer_config.n_layer = 1\n",
        "small_gpt = model.GPT(one_layer_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnrAVnaRgKfQ",
        "outputId": "844d1ccf-5f35-432f-e16d-4f1e77ec7b5d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 7.10M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic train loop on single vector\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedded, mask = torch.from_numpy(tokenized_text[0]),torch.from_numpy(tokenized_text[1])\n",
        "masked_start = mask.clone()\n",
        "masked_start[0,0:3] = False\n",
        "loss = 1 # initial value for loop condition\n",
        "small_gpt.train()\n",
        "small_gpt.to(device)\n",
        "optimizer = small_gpt.configure_optimizers(learning_rate=1e-5, weight_decay=0.0, betas =(0.9,0.95),device_type=device)\n",
        "embedded = embedded.to(device)\n",
        "masked_start = masked_start.to(device)\n",
        "i = 0\n",
        "while loss > 1e-16:\n",
        "  optimizer.zero_grad()\n",
        "  if i%100 == 1:\n",
        "    print(\"Step:\",i,\" Loss:\",loss.item())\n",
        "  pred_logits = small_gpt(embedded,attn_mask=mask)\n",
        "  loss_per_token = torch.nn.functional.cross_entropy(pred_logits.flatten(end_dim=1),embedded.view(-1),reduction=\"none\")\n",
        "  loss = (loss_per_token * masked_start.view(-1)).sum() / masked_start.sum()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  i += 1\n",
        "\n",
        "print(\"Step:\",i,\" Loss:\",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmKWXfH-Q4Gc",
        "outputId": "86e1e9c1-51d7-4784-9171-89f948c175c6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 6, with 7,875,840 parameters\n",
            "num non-decayed parameter tensors: 10, with 11,520 parameters\n",
            "using fused AdamW: True\n",
            "Step: 1  Loss: 2.060918092727661\n",
            "Step: 101  Loss: 0.001617064350284636\n",
            "Step: 201  Loss: 0.00019303396402392536\n",
            "Step: 301  Loss: 2.2637575966655277e-05\n",
            "Step: 401  Loss: 3.856413059111219e-06\n",
            "Step: 501  Loss: 1.2636176052183146e-06\n",
            "Step: 601  Loss: 6.139276820249506e-07\n",
            "Step: 701  Loss: 3.337859197927173e-07\n",
            "Step: 801  Loss: 1.5497205652081902e-07\n",
            "Step: 901  Loss: 1.1324881654672936e-07\n",
            "Step: 1001  Loss: 7.152556946721234e-08\n",
            "Step: 1101  Loss: 5.960463766996327e-08\n",
            "Step: 1201  Loss: 4.172324707951702e-08\n",
            "Step: 1301  Loss: 1.192092824453539e-08\n",
            "Step: 1401  Loss: 5.364417532405241e-08\n",
            "Step: 1403  Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_gpt.eval()\n",
        "with torch.no_grad():\n",
        "  sampling = torch.zeros_like(embedded)\n",
        "  sampling[0,0:3] = embedded[0,0:3]\n",
        "  for i in range(23):\n",
        "    pred_logits = small_gpt(sampling)\n",
        "    sampling[0,i] = pred_logits.argmax(dim=2)[0,i]\n",
        "    print(tokenizer.untokenize(sampling.cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usKUJKDCQ86R",
        "outputId": "d7fce49c-79c7-4f9c-917f-2b111c0a9a35"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['o l']\n",
            "['o l']\n",
            "['o o']\n",
            "['o oo']\n",
            "['o ooo']\n",
            "['o oooo']\n",
            "['o oooo ']\n",
            "['o oooo m']\n",
            "['o oooo mo']\n",
            "['o oooo mom']\n",
            "['o oooo momo']\n",
            "['o oooo momom']\n",
            "['o oooo momomn']\n",
            "['o oooo momomnm']\n",
            "['o oooo momomnmm']\n",
            "['o oooo momomnmmm']\n",
            "['o oooo momomnmmmm']\n",
            "['o oooo momomnmmmmm']\n",
            "['o oooo momomnmmmmmr']\n",
            "['o oooo momomnmmmmmrm']\n",
            "['o oooo momomnmmmmmrmi']\n",
            "['o oooo momomnmmmmmrmim']\n",
            "['o oooo momomnmmmmmrmimm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_gpt.eval()\n",
        "with torch.no_grad():\n",
        "  sampling = torch.zeros_like(embedded)\n",
        "  sampling[0,0:3] = embedded[0,0:3]\n",
        "  for i in range(20):\n",
        "    pred_logits = small_gpt(sampling)\n",
        "    sampling[0,i+3] = pred_logits.argmax(dim=2)[0,i+3]\n",
        "    print(tokenizer.untokenize(sampling.cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFmqwZzUs0oY",
        "outputId": "ae4ccafb-24f7-41f3-c209-e04b46f1c9af"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I lo']\n",
            "['I lov']\n",
            "['I love']\n",
            "['I love ']\n",
            "['I love m']\n",
            "['I love ma']\n",
            "['I love mac']\n",
            "['I love mach']\n",
            "['I love machi']\n",
            "['I love machin']\n",
            "['I love machine']\n",
            "['I love machine ']\n",
            "['I love machine l']\n",
            "['I love machine le']\n",
            "['I love machine lea']\n",
            "['I love machine lear']\n",
            "['I love machine learn']\n",
            "['I love machine learni']\n",
            "['I love machine learnin']\n",
            "['I love machine learning']\n"
          ]
        }
      ]
    }
  ]
}