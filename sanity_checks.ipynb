{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob5IepnmN4PQ",
        "outputId": "1dc0b5e1-65d0-4052-db81-df901de38394"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/hw2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiSyWqjFN6yZ",
        "outputId": "184523d6-9005-45bb-dbe5-c0c8e986b886"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/hw2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import model\n",
        "import utils\n",
        "import train"
      ],
      "metadata": {
        "id": "c2jECcsRJOFe"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHriz22ltPl5",
        "outputId": "6b0fdcc1-e45e-4f5a-dd3d-fdbf91bc7598"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a59197b2ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"I love machine learning\"]\n",
        "tokenizer = utils.Tokenizer(24)\n",
        "tokenizer.build_tokenization(text)\n",
        "tokenized_text = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "rYinPsynNcZj"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_layer_config = model.GPTConfig\n",
        "one_layer_config.vocab_size = tokenizer.vocab_size\n",
        "one_layer_config.n_layer = 1\n",
        "small_gpt = model.GPT(one_layer_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ZvTo0fYUzr",
        "outputId": "d37ab781-cb37-4bf3-a5d4-ad2bd5bfa545"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 7.10M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular inference sampling"
      ],
      "metadata": {
        "id": "JRuPcgbBRdN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic train loop on single vector\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedded, mask = torch.from_numpy(tokenized_text[0]),torch.from_numpy(tokenized_text[1])\n",
        "loss = 1 # initial value for loop condition\n",
        "small_gpt.train()\n",
        "small_gpt.to(device)\n",
        "optimizer = small_gpt.configure_optimizers(learning_rate=1e-5, weight_decay=0.0, betas =(0.9,0.95),device_type=device)\n",
        "embedded = embedded.to(device)\n",
        "mask = mask.to(device)\n",
        "i = 0\n",
        "while loss > 1e-16:\n",
        "  optimizer.zero_grad()\n",
        "  if i%100 == 1:\n",
        "    print(\"Step:\",i,\" Loss:\",loss.item())\n",
        "  pred_logits = small_gpt(embedded)\n",
        "  loss_per_token = torch.nn.functional.cross_entropy(pred_logits.flatten(end_dim=1),embedded.view(-1),reduction=\"none\")\n",
        "  loss = (loss_per_token * mask.view(-1)).sum() / mask.sum()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  i += 1\n",
        "print(\"Step:\",i,\" Loss:\",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys0DMvLo6tZv",
        "outputId": "99f2d7fb-e7d6-4e6b-a7ae-a15302ac975a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 6, with 7,875,840 parameters\n",
            "num non-decayed parameter tensors: 10, with 11,520 parameters\n",
            "using fused AdamW: True\n",
            "Step: 1  Loss: 1.7886563539505005\n",
            "Step: 101  Loss: 0.0017960845725610852\n",
            "Step: 201  Loss: 0.00020159268751740456\n",
            "Step: 301  Loss: 2.3670527298236266e-05\n",
            "Step: 401  Loss: 4.224145868647611e-06\n",
            "Step: 501  Loss: 1.4564252523996402e-06\n",
            "Step: 601  Loss: 7.048894872241362e-07\n",
            "Step: 701  Loss: 3.783598288009671e-07\n",
            "Step: 801  Loss: 2.0213745699493302e-07\n",
            "Step: 901  Loss: 1.0884325263305072e-07\n",
            "Step: 1001  Loss: 8.81112072192991e-08\n",
            "Step: 1101  Loss: 4.6647109286368504e-08\n",
            "Step: 1201  Loss: 3.109807167334111e-08\n",
            "Step: 1301  Loss: 1.5549035836670555e-08\n",
            "Step: 1333  Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_gpt.eval()\n",
        "with torch.no_grad():\n",
        "  sampling = torch.zeros_like(embedded)\n",
        "  sampling[0,0:3] = embedded[0,0:3]\n",
        "  for i in range(23):\n",
        "    pred_logits = small_gpt(sampling)\n",
        "    sampling[0,i] = pred_logits.argmax(dim=2)[0,i]\n",
        "    print(tokenizer.untokenize(sampling.cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmuEoZC2qIRm",
        "outputId": "505716f0-db19-4033-96a3-adf61b3dc973"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I l']\n",
            "['I l']\n",
            "['I l']\n",
            "['I lo']\n",
            "['I lov']\n",
            "['I love']\n",
            "['I love ']\n",
            "['I love m']\n",
            "['I love ma']\n",
            "['I love mac']\n",
            "['I love mach']\n",
            "['I love machi']\n",
            "['I love machin']\n",
            "['I love machine']\n",
            "['I love machine ']\n",
            "['I love machine l']\n",
            "['I love machine le']\n",
            "['I love machine lea']\n",
            "['I love machine lear']\n",
            "['I love machine learn']\n",
            "['I love machine learni']\n",
            "['I love machine learnin']\n",
            "['I love machine learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Same but with first three masked"
      ],
      "metadata": {
        "id": "MeaglVqMQ5Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_layer_config = model.GPTConfig\n",
        "one_layer_config.vocab_size = tokenizer.vocab_size\n",
        "one_layer_config.n_layer = 1\n",
        "small_gpt = model.GPT(one_layer_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnrAVnaRgKfQ",
        "outputId": "2bac397b-01a3-4389-bb39-f821a6908669"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 7.10M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic train loop on single vector\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedded, mask = torch.from_numpy(tokenized_text[0]),torch.from_numpy(tokenized_text[1])\n",
        "masked_start = mask.clone()\n",
        "masked_start[0,0:3] = False\n",
        "loss = 1 # initial value for loop condition\n",
        "small_gpt.train()\n",
        "small_gpt.to(device)\n",
        "optimizer = small_gpt.configure_optimizers(learning_rate=1e-5, weight_decay=0.0, betas =(0.9,0.95),device_type=device)\n",
        "embedded = embedded.to(device)\n",
        "masked_start = masked_start.to(device)\n",
        "i = 0\n",
        "while loss > 1e-16:\n",
        "  optimizer.zero_grad()\n",
        "  if i%100 == 1:\n",
        "    print(\"Step:\",i,\" Loss:\",loss.item())\n",
        "  pred_logits = small_gpt(embedded)\n",
        "  loss_per_token = torch.nn.functional.cross_entropy(pred_logits.flatten(end_dim=1),embedded.view(-1),reduction=\"none\")\n",
        "  loss = (loss_per_token * masked_start.view(-1)).sum() / masked_start.sum()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  i += 1\n",
        "\n",
        "print(\"Step:\",i,\" Loss:\",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmKWXfH-Q4Gc",
        "outputId": "76a264b5-ec61-493d-a342-07c480dfd615"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 6, with 7,875,840 parameters\n",
            "num non-decayed parameter tensors: 10, with 11,520 parameters\n",
            "using fused AdamW: True\n",
            "Step: 1  Loss: 1.7234783172607422\n",
            "Step: 101  Loss: 0.0014911503531038761\n",
            "Step: 201  Loss: 0.00017536508676130325\n",
            "Step: 301  Loss: 2.0843524907832034e-05\n",
            "Step: 401  Loss: 3.898136128555052e-06\n",
            "Step: 501  Loss: 1.3709058066524449e-06\n",
            "Step: 601  Loss: 7.152553962441743e-07\n",
            "Step: 701  Loss: 3.576278118089249e-07\n",
            "Step: 801  Loss: 2.2649760467174929e-07\n",
            "Step: 901  Loss: 1.311302071371756e-07\n",
            "Step: 1001  Loss: 7.152556946721234e-08\n",
            "Step: 1101  Loss: 5.364417532405241e-08\n",
            "Step: 1201  Loss: 3.576278473360617e-08\n",
            "Step: 1301  Loss: 6.55651035685878e-08\n",
            "Step: 1401  Loss: 6.55651035685878e-08\n",
            "Step: 1484  Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_gpt.eval()\n",
        "with torch.no_grad():\n",
        "  sampling = torch.zeros_like(embedded)\n",
        "  sampling[0,0:3] = embedded[0,0:3]\n",
        "  for i in range(23):\n",
        "    pred_logits = small_gpt(sampling)\n",
        "    sampling[0,i] = pred_logits.argmax(dim=2)[0,i]\n",
        "    print(tokenizer.untokenize(sampling.cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usKUJKDCQ86R",
        "outputId": "06cf54e0-815e-4e4e-edd4-7e43b489264e"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a l']\n",
            "['a l']\n",
            "['a o']\n",
            "['a oo']\n",
            "['a ooo']\n",
            "['a oooe']\n",
            "['a oooe ']\n",
            "['a oooe m']\n",
            "['a oooe ma']\n",
            "['a oooe mac']\n",
            "['a oooe mach']\n",
            "['a oooe machi']\n",
            "['a oooe machin']\n",
            "['a oooe machine']\n",
            "['a oooe machine ']\n",
            "['a oooe machine l']\n",
            "['a oooe machine le']\n",
            "['a oooe machine lea']\n",
            "['a oooe machine lear']\n",
            "['a oooe machine learn']\n",
            "['a oooe machine learni']\n",
            "['a oooe machine learnin']\n",
            "['a oooe machine learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_gpt.eval()\n",
        "with torch.no_grad():\n",
        "  sampling = torch.zeros_like(embedded)\n",
        "  sampling[0,0:3] = embedded[0,0:3]\n",
        "  for i in range(20):\n",
        "    pred_logits = small_gpt(sampling)\n",
        "    sampling[0,i+3] = pred_logits.argmax(dim=2)[0,i+3]\n",
        "    print(tokenizer.untokenize(sampling.cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFmqwZzUs0oY",
        "outputId": "85179ec9-8cf6-435d-bae5-8093d2944c1d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I lo']\n",
            "['I lov']\n",
            "['I love']\n",
            "['I love ']\n",
            "['I love m']\n",
            "['I love ma']\n",
            "['I love mac']\n",
            "['I love mach']\n",
            "['I love machi']\n",
            "['I love machin']\n",
            "['I love machine']\n",
            "['I love machine ']\n",
            "['I love machine l']\n",
            "['I love machine le']\n",
            "['I love machine lea']\n",
            "['I love machine lear']\n",
            "['I love machine learn']\n",
            "['I love machine learni']\n",
            "['I love machine learnin']\n",
            "['I love machine learning']\n"
          ]
        }
      ]
    }
  ]
}