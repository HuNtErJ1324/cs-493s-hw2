{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob5IepnmN4PQ",
        "outputId": "430e60d9-df1b-4aee-b81c-8c78b53c8f80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/hw2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiSyWqjFN6yZ",
        "outputId": "e3f5e377-7f94-42b5-92ef-99a0ec4768ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/hw2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import model\n",
        "import utils"
      ],
      "metadata": {
        "id": "c2jECcsRJOFe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"I love machine learning\"]\n",
        "tokenizer = utils.Tokenizer(24)\n",
        "tokenizer.build_tokenization(text)\n",
        "tokenized_text = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "rYinPsynNcZj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_layer_config = model.GPTConfig\n",
        "one_layer_config.vocab_size = tokenizer.vocab_size\n",
        "one_layer_config.n_layer = 1\n",
        "small_gpt = model.GPT(one_layer_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ZvTo0fYUzr",
        "outputId": "289bf89e-fc1a-4b63-c092-76326815b902"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 7.10M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regular inference sampling"
      ],
      "metadata": {
        "id": "JRuPcgbBRdN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic train loop on single vector\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedded, mask = torch.from_numpy(tokenized_text[0]),torch.from_numpy(tokenized_text[1])\n",
        "loss = 1 # initial value for loop condition\n",
        "small_gpt.train()\n",
        "small_gpt.to(device)\n",
        "optimizer = small_gpt.configure_optimizers(learning_rate=1e-5, weight_decay=0.0, betas =(0.9,0.95),device_type=device)\n",
        "embedded = embedded.to(device)\n",
        "mask = mask.to(device)\n",
        "i = 0\n",
        "while loss > 1e-16:\n",
        "  optimizer.zero_grad()\n",
        "  if i%100 == 1:\n",
        "    print(loss.item())\n",
        "  pred_logits = small_gpt(embedded)\n",
        "  loss_per_token = torch.nn.functional.cross_entropy(pred_logits.flatten(end_dim=1),embedded.view(-1),reduction=\"none\")\n",
        "  loss = (loss_per_token * mask.view(-1)).sum() / mask.sum()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  i += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "ys0DMvLo6tZv",
        "outputId": "7849f586-0891-424b-d157-7f7e801809a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 6, with 7,875,840 parameters\n",
            "num non-decayed parameter tensors: 10, with 11,520 parameters\n",
            "using fused AdamW: True\n",
            "1.7325066328048706\n",
            "0.0015796147054061294\n",
            "0.00017355283489450812\n",
            "2.0032135580549948e-05\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-032b96cfdb42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTH2q4Q_P1rW",
        "outputId": "e75bdfd5-d7d5-475c-d8dc-687923b208cb"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_gpt.eval()\n",
        "with torch.no_grad():\n",
        "  sampling = torch.zeros_like(embedded)\n",
        "  sampling[0,0:3] = embedded[0,0:3]\n",
        "  for i in range(20):\n",
        "    pred_logits = small_gpt(sampling)\n",
        "    sampling[0,0:3+i+1] = pred_logits.argmax(dim=2)[0,0:3+i+1]\n",
        "    print(tokenizer.untokenize(sampling.cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUU6MujU_1e-",
        "outputId": "0b9b0861-e274-45c6-a690-c85c8c2a8bf2"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I lo']\n",
            "['I lov']\n",
            "['I love']\n",
            "['I love ']\n",
            "['I love m']\n",
            "['I love ma']\n",
            "['I love mac']\n",
            "['I love mach']\n",
            "['I love machi']\n",
            "['I love machin']\n",
            "['I love machine']\n",
            "['I love machine ']\n",
            "['I love machine l']\n",
            "['I love machine le']\n",
            "['I love machine lea']\n",
            "['I love machine lear']\n",
            "['I love machine learn']\n",
            "['I love machine learni']\n",
            "['I love machine learnin']\n",
            "['I love machine learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Same but with first three masked"
      ],
      "metadata": {
        "id": "MeaglVqMQ5Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic train loop on single vector\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embedded, mask = torch.from_numpy(tokenized_text[0]),torch.from_numpy(tokenized_text[1])\n",
        "masked_start = mask\n",
        "masked_start[0,0:3] = False\n",
        "loss = 1 # initial value for loop condition\n",
        "small_gpt.train()\n",
        "small_gpt.to(device)\n",
        "optimizer = small_gpt.configure_optimizers(learning_rate=1e-5, weight_decay=0.0, betas =(0.9,0.95),device_type=device)\n",
        "embedded = embedded.to(device)\n",
        "masked_start = masked_start.to(device)\n",
        "i = 0\n",
        "while loss > 1e-16:\n",
        "  optimizer.zero_grad()\n",
        "  if i%100 == 1:\n",
        "    print(loss.item())\n",
        "  pred_logits = small_gpt(embedded)\n",
        "  loss_per_token = torch.nn.functional.cross_entropy(pred_logits.flatten(end_dim=1),embedded.view(-1),reduction=\"none\")\n",
        "  loss = (loss_per_token * masked_start.view(-1)).sum() / masked_start.sum()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  i += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmKWXfH-Q4Gc",
        "outputId": "6d9a0687-e975-40da-a7a9-8729f309d82b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 6, with 7,875,840 parameters\n",
            "num non-decayed parameter tensors: 10, with 11,520 parameters\n",
            "using fused AdamW: True\n",
            "1.6450914144515991\n",
            "0.0013822962064296007\n",
            "0.00016000140749383718\n",
            "1.8662036382011138e-05\n",
            "3.3855380934255663e-06\n",
            "1.1682502645271597e-06\n",
            "5.781648724223487e-07\n",
            "3.159045718348352e-07\n",
            "1.7881390590446244e-07\n",
            "1.0728835064810482e-07\n",
            "5.960463766996327e-08\n",
            "4.172324707951702e-08\n",
            "4.768370942542788e-08\n",
            "5.364417532405241e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_gpt.eval()\n",
        "with torch.no_grad():\n",
        "  sampling = torch.zeros_like(embedded)\n",
        "  sampling[0,0:3] = embedded[0,0:3]\n",
        "  print(tokenizer.untokenize(sampling.cpu().numpy()))\n",
        "  for i in range(20):\n",
        "    pred_logits = small_gpt(sampling)\n",
        "    sampling[0,0:3+i+1] = pred_logits.argmax(dim=2)[0,0:3+i+1]\n",
        "    print(tokenizer.untokenize(sampling.cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usKUJKDCQ86R",
        "outputId": "64ba25fd-8334-4b1d-f76e-9bdcc58a13c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I l']\n",
            "['o oo']\n",
            "['o oov']\n",
            "['o oove']\n",
            "['o oovee']\n",
            "['o ooveem']\n",
            "['o ooveeme']\n",
            "['o ooveemee']\n",
            "['o ooveemeee']\n",
            "['o ooveemeeei']\n",
            "['o ooveemeeeie']\n",
            "['o ooveemeeeiee']\n",
            "['o ooveemeeeieee']\n",
            "['o ooveemeeeieeee']\n",
            "['o ooveemeeeieeeee']\n",
            "['o ooveemeeeieeeeee']\n",
            "['o ooveemeeeieeeeeee']\n",
            "['o ooveemeeeieeeeeeee']\n",
            "['o ooveemeeeieeeeeeeee']\n",
            "['o ooveemeeeieeeeeeeeee']\n",
            "['o ooveemeeeieeeeeeeeeee']\n"
          ]
        }
      ]
    }
  ]
}